{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MY_DAY_DEEP_DREAM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4ESjSAM99T8cExEtthmQb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abishekravi/DEEPLEARNING/blob/master/MY_DAY_DEEP_DREAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLY42xH_kF6s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e74a732a-c80b-43fb-b764-5404fd37ea46"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQctC4mphmrW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "2207a75e-179b-4f4d-8353-47032431ed3a"
      },
      "source": [
        "!wget -nc --no-check-certificate https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip && unzip -n inception5h.zip\n",
        "!wget -nc https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg\n",
        "#path=\"YellowLabradorLooking_new.jpg\"\n",
        "path=\"003.jpg\"\n",
        "with open(path, 'rb') as f:\n",
        "  file_contents = f.read()\n",
        "#file_contents = open(\"YellowLabradorLooking_new.jpg\").read()  \n",
        "\n",
        "from io import BytesIO\n",
        "from IPython.display import clear_output, Image, display\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfc\n",
        "from __future__ import print_function\n",
        "\n",
        "model_fn = 'tensorflow_inception_graph.pb'\n",
        "\n",
        "# creating TensorFlow session and loading the model\n",
        "graph = tf.Graph()\n",
        "sess = tfc.InteractiveSession(graph=graph)\n",
        "with tfc.gfile.FastGFile(model_fn, 'rb') as f:\n",
        "    graph_def = tfc.GraphDef()\n",
        "    graph_def.ParseFromString(f.read())\n",
        "t_input = tfc.placeholder(np.float32, name='input') # define the input tensor\n",
        "imagenet_mean = 117.0\n",
        "t_preprocessed = tfc.expand_dims(t_input-imagenet_mean, 0)\n",
        "tfc.import_graph_def(graph_def, {'input':t_preprocessed})\n",
        "\n",
        "def T(layer):\n",
        "    '''Helper for getting layer output tensor'''\n",
        "    return graph.get_tensor_by_name(\"import/%s:0\"%layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘inception5h.zip’ already there; not retrieving.\n",
            "\n",
            "Archive:  inception5h.zip\n",
            "File ‘YellowLabradorLooking_new.jpg’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1752: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFbr3A9ghoRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuvY-T0DlYiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if type(uploaded) is not dict: uploaded = uploaded.files  ## Deal with filedit versions\n",
        "file_contents = uploaded[uploaded.keys()[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wkOPJb-lZTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showarray(a, fmt='jpeg'):\n",
        "    a = np.uint8(np.clip(a, 0, 255))\n",
        "    f = BytesIO()\n",
        "    PIL.Image.fromarray(a).save(f, fmt)\n",
        "    display(Image(data=f.getvalue()))\n",
        "img0 = sess.run(tf.image.decode_image(file_contents))\n",
        "showarray(img0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqdJVhk0mYmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These parameters let us control the strenth of the deepdream.\n",
        "octave_n = 4\n",
        "octave_scale = 1.4\n",
        "iter_n = 10\n",
        "strength = 200\n",
        "\n",
        "# Helper function that uses TensorFlow to resize an image\n",
        "def resize(img, new_size):\n",
        "    return sess.run(tfc.image.resize_bilinear(img[np.newaxis,:], new_size))[0]\n",
        "\n",
        "# Apply gradients to an image in a seires of tiles\n",
        "def calc_grad_tiled(img, t_grad, tile_size=256):\n",
        "    '''Random shifts are applied to the image to blur tile boundaries over\n",
        "    multiple iterations.'''\n",
        "    h, w = img.shape[:2]\n",
        "    sx, sy = np.random.randint(tile_size, size=2)\n",
        "    # We randomly roll the image in x and y to avoid seams between tiles.\n",
        "    img_shift = np.roll(np.roll(img, sx, 1), sy, 0)\n",
        "    grad = np.zeros_like(img)\n",
        "    for y in range(0, max(h-tile_size//2, tile_size),tile_size):\n",
        "        for x in range(0, max(w-tile_size//2, tile_size),tile_size):\n",
        "            sub = img_shift[y:y+tile_size,x:x+tile_size]\n",
        "            g = sess.run(t_grad, {t_input:sub})\n",
        "            grad[y:y+tile_size,x:x+tile_size] = g\n",
        "    imggrad = np.roll(np.roll(grad, -sx, 1), -sy, 0)\n",
        "    # Add the image gradient to the image and return the result\n",
        "    return img + imggrad*(strength * 0.01 / (np.abs(imggrad).mean()+1e-7))\n",
        "\n",
        "# Applies deepdream at multiple scales\n",
        "def render_deepdream(t_obj, input_img, show_steps = True):\n",
        "    # Collapse the optimization objective to a single number (the loss)\n",
        "    t_score = tfc.reduce_mean(t_obj)\n",
        "    # We need the gradient of the image with respect to the objective\n",
        "    t_grad = tfc.gradients(t_score, t_input)[0]\n",
        "\n",
        "    # split the image into a number of octaves (laplacian pyramid)\n",
        "    img = input_img\n",
        "    octaves = []\n",
        "    for i in range(octave_n-1):\n",
        "        lo = resize(img, np.int32(np.float32(img.shape[:2])/octave_scale))\n",
        "        octaves.append(img-resize(lo, img.shape[:2]))\n",
        "        img = lo\n",
        "\n",
        "    # generate details octave by octave\n",
        "    for octave in range(octave_n):\n",
        "        if octave>0:\n",
        "            hi = octaves[-octave]\n",
        "            img = resize(img, hi.shape[:2])+hi\n",
        "        for i in range(iter_n):\n",
        "            img = calc_grad_tiled(img, t_grad)\n",
        "        if show_steps:\n",
        "            clear_output()\n",
        "            showarray(img)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sENCQrwPmdaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "octave_n = 4 #@param {type:\"slider\", max: 10}\n",
        "octave_scale = 1.4 #@param {type:\"number\"}\n",
        "iter_n = 17 #@param {type:\"slider\", max: 50}\n",
        "strength = 193 #@param {type:\"slider\", max: 1000}\n",
        "layer = \"mixed5a\"  #@param [\"mixed3a\", \"mixed3b\", \"mixed4a\", \"mixed4c\", \"mixed5a\"]\n",
        "\n",
        "final = render_deepdream(tfc.square(T(layer)), img0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZeuZnyimhHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_channel = 38 #@param {type:\"slider\", max: 512}\n",
        "layer = \"mixed4d_3x3_bottleneck_pre_relu\"  #@param [\"mixed4d_3x3_bottleneck_pre_relu\", \"mixed3a\", \"mixed3b\", \"mixed4a\", \"mixed4c\", \"mixed5a\"]\n",
        "if feature_channel >= T(layer).shape[3]:\n",
        "  print(\"Feature channel exceeds size of layer \", layer, \" feature space. \")\n",
        "  print(\"Choose a smaller channel number.\")\n",
        "else:\n",
        "  render_deepdream(T(layer)[:,:,:,feature_channel], img0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NheQuJg8qM7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer = \"mixed4d_3x3_bottleneck_pre_relu\"  #@param [\"mixed4d_3x3_bottleneck_pre_relu\", \"mixed3a\", \"mixed3b\", \"mixed4a\", \"mixed4c\", \"mixed5a\"]\n",
        "iter_n = 5 #@param {type:\"slider\", max: 50}\n",
        "strength = 150 #@param {type:\"slider\", max: 1000}\n",
        "zooming_steps = 20 #@param {type:\"slider\", max: 512}\n",
        "zoom_factor = 1.1 #@param {type:\"number\"}\n",
        "\n",
        "frame = img0\n",
        "img_y, img_x, _ = img0.shape\n",
        "for i in range(zooming_steps):\n",
        "  frame = render_deepdream(tf.square(T(layer)), frame, False)\n",
        "  clear_output()\n",
        "  showarray(frame)\n",
        "  newsize = np.int32(np.float32(frame.shape[:2])*zoom_factor)\n",
        "  frame = resize(frame, newsize)\n",
        "  frame = frame[(newsize[0]-img_y)//2:(newsize[0]-img_y)//2+img_y,\n",
        "                (newsize[1]-img_x)//2:(newsize[1]-img_x)//2+img_x,:]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnF7gE_9qciY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e0dbeb2-0f19-4047-bb13-22de9e2f7a15"
      },
      "source": [
        "layers = [op.name for op in graph.get_operations() if op.type=='Conv2D' and 'import/' in op.name]\n",
        "feature_nums = [int(graph.get_tensor_by_name(name+':0').get_shape()[-1]) for name in layers]\n",
        "\n",
        "print('Number of layers', len(layers))\n",
        "print('Total number of feature channels:', sum(feature_nums))\n",
        "\n",
        "for layer in layers:\n",
        "  print('Layer:', layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of layers 59\n",
            "Total number of feature channels: 7548\n",
            "Layer: import/conv2d0_pre_relu/conv\n",
            "Layer: import/conv2d1_pre_relu/conv\n",
            "Layer: import/conv2d2_pre_relu/conv\n",
            "Layer: import/mixed3a_1x1_pre_relu/conv\n",
            "Layer: import/mixed3a_3x3_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed3a_3x3_pre_relu/conv\n",
            "Layer: import/mixed3a_5x5_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed3a_5x5_pre_relu/conv\n",
            "Layer: import/mixed3a_pool_reduce_pre_relu/conv\n",
            "Layer: import/mixed3b_1x1_pre_relu/conv\n",
            "Layer: import/mixed3b_3x3_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed3b_3x3_pre_relu/conv\n",
            "Layer: import/mixed3b_5x5_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed3b_5x5_pre_relu/conv\n",
            "Layer: import/mixed3b_pool_reduce_pre_relu/conv\n",
            "Layer: import/mixed4a_1x1_pre_relu/conv\n",
            "Layer: import/mixed4a_3x3_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4a_3x3_pre_relu/conv\n",
            "Layer: import/mixed4a_5x5_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4a_5x5_pre_relu/conv\n",
            "Layer: import/mixed4a_pool_reduce_pre_relu/conv\n",
            "Layer: import/mixed4b_1x1_pre_relu/conv\n",
            "Layer: import/mixed4b_3x3_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4b_3x3_pre_relu/conv\n",
            "Layer: import/mixed4b_5x5_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4b_5x5_pre_relu/conv\n",
            "Layer: import/mixed4b_pool_reduce_pre_relu/conv\n",
            "Layer: import/mixed4c_1x1_pre_relu/conv\n",
            "Layer: import/mixed4c_3x3_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4c_3x3_pre_relu/conv\n",
            "Layer: import/mixed4c_5x5_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4c_5x5_pre_relu/conv\n",
            "Layer: import/mixed4c_pool_reduce_pre_relu/conv\n",
            "Layer: import/mixed4d_1x1_pre_relu/conv\n",
            "Layer: import/mixed4d_3x3_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4d_3x3_pre_relu/conv\n",
            "Layer: import/mixed4d_5x5_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4d_5x5_pre_relu/conv\n",
            "Layer: import/mixed4d_pool_reduce_pre_relu/conv\n",
            "Layer: import/mixed4e_1x1_pre_relu/conv\n",
            "Layer: import/mixed4e_3x3_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4e_3x3_pre_relu/conv\n",
            "Layer: import/mixed4e_5x5_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed4e_5x5_pre_relu/conv\n",
            "Layer: import/mixed4e_pool_reduce_pre_relu/conv\n",
            "Layer: import/mixed5a_1x1_pre_relu/conv\n",
            "Layer: import/mixed5a_3x3_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed5a_3x3_pre_relu/conv\n",
            "Layer: import/mixed5a_5x5_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed5a_5x5_pre_relu/conv\n",
            "Layer: import/mixed5a_pool_reduce_pre_relu/conv\n",
            "Layer: import/mixed5b_1x1_pre_relu/conv\n",
            "Layer: import/mixed5b_3x3_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed5b_3x3_pre_relu/conv\n",
            "Layer: import/mixed5b_5x5_bottleneck_pre_relu/conv\n",
            "Layer: import/mixed5b_5x5_pre_relu/conv\n",
            "Layer: import/mixed5b_pool_reduce_pre_relu/conv\n",
            "Layer: import/head0_bottleneck_pre_relu/conv\n",
            "Layer: import/head1_bottleneck_pre_relu/conv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1JXOG_jqqPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer = \"mixed4a_3x3_pre_relu\"\n",
        "final = render_deepdream(tf.square(T(layer)), img0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}